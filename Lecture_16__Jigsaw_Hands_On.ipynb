{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUsQh9dnx4Et"
      },
      "source": [
        "###  MC959-MO810 -- Introduction to Self-Supervised (Representation) Learning (SSRL)\n",
        "\n",
        "* Instructor: Marcelo S. Reis. <a href=\"mailto:msreis@unicamp.br\">msreis@unicamp.br</a>\n",
        "\n",
        "Campinas, October 9, 2024.\n",
        "\n",
        "\n",
        "## Lecture 16: Jigsaw Hands On\n",
        "\n",
        "*Based on ['Unsupervised Jigsaw Puzzle Solving on MNIST'](https://www.kaggle.com/code/kmader/unsupervised-jigsaw-puzzle-solving-on-mnist/notebook) by Scott Mader.*\n",
        "\n",
        "In this lab, we will carry out a SSRL pipeline using the Jigsaw method.\n",
        "\n",
        "<img src=\"http://www.ic.unicamp.br/~msreis/MC959/Jigsaw-01.png\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90AeZi_ax4Ev"
      },
      "source": [
        "### Summary <a class=\"anchor\" id=\"topo\"></a>\n",
        "\n",
        "* [Part 1: Solving main dependencies](#part_01).\n",
        "* [Part 2: Loading the MNIST dataset](#part_02).\n",
        "* [Part 3: Implementing the Jigsaw scramble and unscramble](#part_03).\n",
        "* [Part 4: Generating permutations of the patches](#part_04).\n",
        "* [Part 5: Implementing the Jigsaw model](#part_05).\n",
        "* [Part 6: Self-supervised pretraining with the Jigsaw and MNIST](#part_06).\n",
        "* [Part 7: Exploration of the learned representations](#part_07)\n",
        "* [Part 8: Evaluating the downstream task (digit classification)](#part_08).\n",
        "* [Part 9: Suggestion of exercises on this pipeline](#part_09).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBmMug7Sx4Ew"
      },
      "source": [
        "### Part 1: Solving main dependencies <a class=\"anchor\" id=\"part_01\"></a>\n",
        "\n",
        "Here we load the main libraries that will be used throughout this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "_kg_hide-input": true,
        "id": "szen8MJBx4Ex"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.24.0)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (3.4.1)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (9.2.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.36.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2024.9.20)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (23.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (0.4)\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "!pip install scikit-image\n",
        "from skimage.io import imread as imread\n",
        "from skimage.util import montage\n",
        "\n",
        "from itertools import product  # Cartesian product between values\n",
        "                               # in x and y (for efficient for loops).\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "from IPython.display import clear_output\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
        "plt.rcParams[\"figure.dpi\"] = 125\n",
        "plt.rcParams[\"font.size\"] = 14\n",
        "plt.rcParams['font.family'] = ['sans-serif']\n",
        "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
        "plt.style.use('ggplot')\n",
        "sns.set_style(\"whitegrid\", {'axes.grid': False})\n",
        "plt.rcParams['image.cmap'] = 'gray' # grayscale looks better\n",
        "\n",
        "from itertools import cycle\n",
        "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
        "colors = prop_cycle.by_key()['color']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ygiF2pjYx4Ey"
      },
      "outputs": [],
      "source": [
        "# Path to the folder where the datasets are/should be downloaded (e.g. )\n",
        "#\n",
        "DATASET_PATH = \"./data/\"  # Using the same data downloaded for Lecture 12\n",
        "\n",
        "# MNIST data in CSV can be retrieved from here:\n",
        "#\n",
        "# https://www.kaggle.com/datasets/oddrationale/mnist-in-csv?select=mnist_test.csv\n",
        "\n",
        "MY_SEED = 42\n",
        "\n",
        "np.random.seed(MY_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": true,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "fMWTyfv9x4Ez"
      },
      "source": [
        "[Back to summary.](#topo)\n",
        "\n",
        "\n",
        "### Part 2: Loading the MNIST dataset <a class=\"anchor\" id=\"part_02\"></a>\n",
        "\n",
        "Loading the classic MNIST dataset for digit classification. Each pair $(x,y)$ is $28 \\times 28$-pixel, grayscale handwritten digit and its respective digit value. The dataset has 60K and 10K observations for training and test, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "_kg_hide-input": true,
        "id": "ZkHn0-nLx4E0"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './data/mnist_train.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train   \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASET_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmnist_train.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m y_train \u001b[38;5;241m=\u001b[39m train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m train   \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mdrop(labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m], axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/mnist_train.csv'"
          ]
        }
      ],
      "source": [
        "\n",
        "train   = pd.read_csv(DATASET_PATH + \"mnist_train.csv\", index_col =  False)\n",
        "y_train = train[\"label\"]\n",
        "train   = train.drop(labels = [\"label\"], axis = 1)\n",
        "train   = train / 255.0   # Scaling in the range [0,1].\n",
        "\n",
        "test   = pd.read_csv(DATASET_PATH + \"mnist_test.csv\", index_col = False)\n",
        "y_test = test[\"label\"]\n",
        "test   = test.drop(labels = [\"label\"], axis = 1)\n",
        "test   = test / 255.0\n",
        "\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sO-PuXVQx4E1"
      },
      "outputs": [],
      "source": [
        "X_train = []\n",
        "X_val   = []\n",
        "X_down  = []\n",
        "\n",
        "for i in range(0,len(train)):\n",
        "    # Reshape image in 3 dimensions\n",
        "    # (height = 28px, width = 28px , channel = 1)\n",
        "    #\n",
        "    image = np.array(train.iloc[[i]]).reshape(-1,28,28).squeeze()\n",
        "    image = np.expand_dims(image, -1)\n",
        "    X_train.append(image)\n",
        "\n",
        "for i in range(0,len(test)):\n",
        "    # Reshape image in 3 dimensions\n",
        "    # (height = 28px, width = 28px , channel = 1)\n",
        "    #\n",
        "    image = np.array(test.iloc[[i]]).reshape(-1,28,28).squeeze()\n",
        "    image = np.expand_dims(image, -1)\n",
        "    X_down.append(image)\n",
        "\n",
        "# del train  # They should be removed to save space.\n",
        "# del test\n",
        "\n",
        "X_val   = np.array(X_train[40000:60001])\n",
        "X_train = np.array(X_train[0:40000])\n",
        "y_train = np.array(y_train[0:40000])     # We'll keep for a visualization later.\n",
        "X_down  = np.array(X_down)\n",
        "y_down  = np.array(y_test)\n",
        "\n",
        "print('Pretext task training data  (no label)  : ' , X_train.shape)\n",
        "print('Pretext task validation data (no label) : ', X_val.shape)\n",
        "print('Downstream task data (with labels)      : ', X_down.shape, y_down.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhnfdqA0x4E1"
      },
      "outputs": [],
      "source": [
        "\n",
        "img_idx = np.random.choice(range(X_train.shape[0]), size=1)\n",
        "plt.matshow(X_train[img_idx].squeeze())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESWun_bDx4E2"
      },
      "source": [
        "[Back to summary.](#topo)\n",
        "\n",
        "### Part 3: Implementing the Jigsaw scramble and unscramble <a class=\"anchor\" id=\"part_04\"></a>\n",
        "\n",
        "Here we implement a function to break up the image into (typically nine) patches (```cut_jigsaw```), and also another one to bring them back into a recoverd image (```jigsaw_to_image```).\n",
        "\n",
        "*Disclaimer: This implementation is for didactic purposes, and it is not optimized to run efficiently on large datasets.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byI7KY55x4E3"
      },
      "outputs": [],
      "source": [
        "import doctest\n",
        "import copy\n",
        "import functools\n",
        "\n",
        "# Unit tests within the functions.\n",
        "#\n",
        "def autotest(func):\n",
        "    globs = copy.copy(globals())\n",
        "    globs.update({func.__name__: func})\n",
        "    doctest.run_docstring_examples(func, globs, verbose=True, name=func.__name__)\n",
        "    return func\n",
        "\n",
        "@autotest\n",
        "def cut_jigsaw(in_image, x_wid, y_wid, gap = False, jitter = False, jitter_dim = None):\n",
        "\n",
        "    \"\"\"Cuts the image into little pieces\n",
        "    in_image   : the image to cut-apart\n",
        "    x_wid      : the size of the piece in x\n",
        "    y_wid      : the size of the piece in y\n",
        "    gap        : if there is a gap between tiles\n",
        "    jitter     : if the positions should be moved around\n",
        "    jitter_dim : amount to jitter (default is x_wid or y_wid/2)\n",
        "    return     : a 4D array with tiles x x_wid x y_wid * d\n",
        "\n",
        "    Examples\n",
        "\n",
        "    >>> test_image = np.arange(20).reshape((4, 5)).astype(int)\n",
        "    >>> test_image\n",
        "    array([[ 0,  1,  2,  3,  4],\n",
        "           [ 5,  6,  7,  8,  9],\n",
        "           [10, 11, 12, 13, 14],\n",
        "           [15, 16, 17, 18, 19]])\n",
        "    >>> cut_jigsaw(test_image, 2, 2, False, False)\n",
        "    array([[[ 0,  1],\n",
        "            [ 5,  6]],\n",
        "    <BLANKLINE>\n",
        "           [[ 2,  3],\n",
        "            [ 7,  8]],\n",
        "    <BLANKLINE>\n",
        "           [[10, 11],\n",
        "            [15, 16]],\n",
        "    <BLANKLINE>\n",
        "           [[12, 13],\n",
        "            [17, 18]]])\n",
        "    >>> cut_jigsaw(test_image, 2, 2, True, False)\n",
        "    array([[[ 0,  1],\n",
        "            [ 5,  6]],\n",
        "    <BLANKLINE>\n",
        "           [[ 3,  4],\n",
        "            [ 8,  9]],\n",
        "    <BLANKLINE>\n",
        "           [[10, 11],\n",
        "            [15, 16]],\n",
        "    <BLANKLINE>\n",
        "           [[13, 14],\n",
        "            [18, 19]]])\n",
        "    >>> np.random.seed(0)\n",
        "    >>> cut_jigsaw(test_image, 2, 2, True, True, 1)\n",
        "    array([[[ 1,  2],\n",
        "            [ 6,  7]],\n",
        "    <BLANKLINE>\n",
        "           [[ 7,  8],\n",
        "            [12, 13]],\n",
        "    <BLANKLINE>\n",
        "           [[ 5,  6],\n",
        "            [10, 11]],\n",
        "    <BLANKLINE>\n",
        "           [[ 7,  8],\n",
        "            [12, 13]]])\n",
        "    \"\"\"\n",
        "\n",
        "    if len(in_image.shape) == 2:\n",
        "        in_image = np.expand_dims(in_image, -1)  # If the image is w x h, expand the shape to turn it w x h x 1.\n",
        "        expand = True\n",
        "    else:\n",
        "        expand = False                           # It already has at least one channel.\n",
        "\n",
        "    x_size, y_size, d_size = in_image.shape\n",
        "    out_tiles = []\n",
        "    x_chunks = x_size // x_wid\n",
        "    y_chunks = y_size // y_wid\n",
        "    out_tiles = np.zeros((x_chunks * y_chunks, x_wid, y_wid, d_size), dtype = in_image.dtype)\n",
        "\n",
        "    if gap:\n",
        "        x_gap = x_size - x_chunks * x_wid    # Maximum gap size.\n",
        "        y_gap = y_size - y_chunks * y_wid\n",
        "    else:\n",
        "        x_gap, y_gap = 0, 0\n",
        "\n",
        "    x_jitter = x_wid // 2 if jitter_dim is None else jitter_dim\n",
        "    y_jitter = y_wid // 2 if jitter_dim is None else jitter_dim\n",
        "\n",
        "    for idx, (i, j) in enumerate(product(range(x_chunks), range(y_chunks))):\n",
        "\n",
        "        x_start = i * x_wid + min(x_gap, i)\n",
        "        y_start = j * y_wid + min(y_gap, j)\n",
        "\n",
        "        if jitter:\n",
        "            x_range = max(x_start - x_jitter, 0), min(x_start + x_jitter + 1, x_size - x_wid)\n",
        "            y_range = max(y_start - y_jitter, 0), min(y_start + y_jitter + 1, y_size - y_wid)\n",
        "\n",
        "            x_start = np.random.choice(range(*x_range)) if x_range[1] > x_range[0] else x_start\n",
        "            y_start = np.random.choice(range(*y_range)) if y_range[1] > y_range[0] else y_start\n",
        "\n",
        "        out_tiles[idx, :, :, :] = in_image[x_start:x_start+x_wid, y_start:y_start+y_wid,:]\n",
        "\n",
        "    return out_tiles[:, :, :, 0] if expand else out_tiles       # type: (...) -> List[np.ndarray]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aL8VGnX6x4E3"
      },
      "outputs": [],
      "source": [
        "@autotest\n",
        "def jigsaw_to_image(in_tiles, out_x, out_y, gap = False):\n",
        "\n",
        "    \"\"\"Reassembles little pieces into an image\n",
        "    in_tiles : the tiles to reassemble\n",
        "    out_x    : the size of the image in x (default is calculated automatically)\n",
        "    out_y    : the size of the image in y\n",
        "    gap      : if there is a gap between tiles\n",
        "    return   : an image from the tiles\n",
        "\n",
        "    Examples\n",
        "\n",
        "    >>> test_image = np.arange(20).reshape((4, 5)).astype(int)\n",
        "    >>> test_image\n",
        "    array([[ 0,  1,  2,  3,  4],\n",
        "           [ 5,  6,  7,  8,  9],\n",
        "           [10, 11, 12, 13, 14],\n",
        "           [15, 16, 17, 18, 19]])\n",
        "    >>> js_pieces = cut_jigsaw(test_image, 2, 2, False, False)\n",
        "    >>> jigsaw_to_image(js_pieces, 4, 5)\n",
        "    array([[ 0,  1,  2,  3,  0],\n",
        "           [ 5,  6,  7,  8,  0],\n",
        "           [10, 11, 12, 13,  0],\n",
        "           [15, 16, 17, 18,  0]])\n",
        "    >>> js_gap_pieces = cut_jigsaw(test_image, 2, 2, True, False)\n",
        "    >>> jigsaw_to_image(js_gap_pieces, 4, 5, True)\n",
        "    array([[ 0,  1,  0,  3,  4],\n",
        "           [ 5,  6,  0,  8,  9],\n",
        "           [10, 11,  0, 13, 14],\n",
        "           [15, 16,  0, 18, 19]])\n",
        "    >>> js_gap_pieces = cut_jigsaw(test_image, 2, 2, False, True)\n",
        "    >>> jigsaw_to_image(js_gap_pieces, 4, 5, False)\n",
        "    array([[ 1,  2,  6,  7,  0],\n",
        "           [ 6,  7, 11, 12,  0],\n",
        "           [ 6,  7,  7,  8,  0],\n",
        "           [11, 12, 12, 13,  0]])\n",
        "    \"\"\"\n",
        "    if len(in_tiles.shape) == 3:\n",
        "        in_tiles = np.expand_dims(in_tiles, -1)\n",
        "        expand = True\n",
        "    else:\n",
        "        expand = False\n",
        "\n",
        "    tile_count, x_wid, y_wid, d_size = in_tiles.shape\n",
        "\n",
        "    x_chunks = out_x // x_wid\n",
        "    y_chunks = out_y // y_wid\n",
        "    out_image = np.zeros((out_x, out_y, d_size), dtype = in_tiles.dtype)\n",
        "\n",
        "    if gap:\n",
        "        x_gap = out_x - x_chunks * x_wid\n",
        "        y_gap = out_y - y_chunks * y_wid\n",
        "    else:\n",
        "        x_gap, y_gap = 0, 0\n",
        "\n",
        "    for idx, (i, j) in enumerate(product(range(x_chunks), range(y_chunks))):\n",
        "        x_start = i * x_wid + min(x_gap, i)\n",
        "        y_start = j * y_wid + min(y_gap, j)\n",
        "        out_image[x_start:x_start+x_wid, y_start:y_start+y_wid] = in_tiles[idx, :, :]\n",
        "\n",
        "    return out_image[:, :, 0] if expand else out_image     # type: (...) -> np.ndarray\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qeWVvpIx4E4"
      },
      "source": [
        "Let's put those two functions into action on some MNIST images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nwaGhbHx4E4"
      },
      "outputs": [],
      "source": [
        "TILE_X = 9\n",
        "TILE_Y = 9\n",
        "\n",
        "out_tiles = cut_jigsaw(np.array(X_train[0]), TILE_X, TILE_Y, gap=False)\n",
        "out_tiles.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFbj07jPx4E4"
      },
      "outputs": [],
      "source": [
        "fig, m_axs = plt.subplots(4, 11, figsize=(50, 10))\n",
        "\n",
        "for img_idx, c_axs in enumerate(m_axs, 1):\n",
        "\n",
        "    c_axs[0].imshow(X_train[img_idx])\n",
        "    c_axs[0].set_title('Input')\n",
        "\n",
        "    out_tiles = cut_jigsaw(X_train[img_idx], TILE_X, TILE_Y, gap=False)\n",
        "\n",
        "    for k, c_ax in zip(range(out_tiles.shape[0]), c_axs[1:]):\n",
        "        c_ax.matshow(out_tiles[k, :, :])\n",
        "\n",
        "    recon_img = jigsaw_to_image(out_tiles, X_train.shape[1], X_train.shape[2])\n",
        "    c_axs[-1].imshow(recon_img[:, :])\n",
        "    c_axs[-1].set_title('Reconstruction')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_KbqIfmx4E5"
      },
      "source": [
        "[Back to summary.](#topo)\n",
        "\n",
        "\n",
        "### Part 4: Generating permutations of the patches <a class=\"anchor\" id=\"part_04\"></a>\n",
        "\n",
        "For a $3 \\times 3$ grid of patches, we have $9!$ possible permutations. As we discussed in Lecture 15, this would be a problem for the pretext task, since there are far too many permutations to be predicted, which spoil from both computational and statistical points of view.\n",
        "\n",
        "Therefore, we'll keep just a few of all possible permutations (e.g., 200 of them).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EVzag7Mx4E5"
      },
      "outputs": [],
      "source": [
        "\n",
        "KEEP_RANDOM_PERM = 200\n",
        "\n",
        "from itertools import permutations\n",
        "all_perm = np.array(list(permutations(range(out_tiles.shape[0]), out_tiles.shape[0])))\n",
        "print('Permutation count:' , len(all_perm))\n",
        "\n",
        "# first one is always unmessed up\n",
        "#\n",
        "keep_perm = all_perm[0:1, :].tolist() \\\n",
        "+ all_perm[np.random.choice(range(1, len(all_perm)), KEEP_RANDOM_PERM-1), :].tolist()\n",
        "\n",
        "print('Permutations kept: ', len(keep_perm))\n",
        "\n",
        "keep_perm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehLzQaorx4E5"
      },
      "source": [
        "Let's visualize some permutations in action on images, looking at the effect of applying jitter on them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TnptNJDx4E6"
      },
      "outputs": [],
      "source": [
        "JITTER_SIZE = 3\n",
        "\n",
        "fig, m_axs = plt.subplots(5, 5, figsize=(15, 25))\n",
        "\n",
        "an_image = X_train[666]\n",
        "\n",
        "for i, c_axs in enumerate(m_axs.T):\n",
        "\n",
        "    out_tiles = cut_jigsaw(an_image, TILE_X, TILE_Y, gap=False, jitter=i>0, jitter_dim=JITTER_SIZE)\n",
        "\n",
        "    for j, (c_ax, c_perm) in enumerate(zip(c_axs, keep_perm)):\n",
        "        scrambled_tiles = out_tiles[c_perm]\n",
        "        recon_img = jigsaw_to_image(scrambled_tiles, X_train.shape[1], X_train.shape[2])\n",
        "        c_ax.imshow(recon_img)\n",
        "        c_ax.set_title('Permutation:#{}\\nJitter:{}'.format(j, i>0))\n",
        "        c_ax.axis('off')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNM_VjYmx4E6"
      },
      "source": [
        "Finally, we'll use the scramble function and the permutation scheme to pre-process the training and validation data, making them ready for the self-supervised learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sRhAtjjx4E6"
      },
      "outputs": [],
      "source": [
        "TRAIN_TILE_COUNT = 2 ** 14  # 16,384\n",
        "VALID_TILE_COUNT = 2 ** 11  #  2,048\n",
        "\n",
        "def _generate_batch(in_idx, is_validation = False):\n",
        "\n",
        "    if is_validation:\n",
        "        img_ds = X_val\n",
        "    else:\n",
        "        img_ds = X_train\n",
        "\n",
        "    img_idx = np.random.choice(range(img_ds.shape[0]))  # Takes a single random image\n",
        "\n",
        "    out_tiles = cut_jigsaw(img_ds[img_idx], TILE_X, TILE_Y, gap = True,\n",
        "                           jitter = JITTER_SIZE > 0, jitter_dim = JITTER_SIZE)\n",
        "\n",
        "    perm_idx = np.random.choice(range(len(keep_perm)))\n",
        "    c_perm = keep_perm[perm_idx]\n",
        "\n",
        "    return out_tiles[c_perm], perm_idx\n",
        "\n",
        "\n",
        "def make_tile_group(out_tiles_shape, tile_count, is_validation = False):\n",
        "\n",
        "    c_tiles = np.zeros((tile_count,) + out_tiles_shape, dtype='float32')\n",
        "\n",
        "    c_perms = np.zeros((tile_count,), dtype='int')\n",
        "\n",
        "    for i in tqdm_notebook(range(tile_count)):\n",
        "        c_tiles[i], c_perms[i] = _generate_batch(i, is_validation = is_validation)     # should be parallelized\n",
        "\n",
        "    return c_tiles, c_perms\n",
        "\n",
        "\n",
        "# Preparating training and validation data.\n",
        "#\n",
        "out_tiles_shape = cut_jigsaw(X_train[8].squeeze(), TILE_X, TILE_Y, gap=False)\n",
        "out_tiles_shape = out_tiles.shape\n",
        "\n",
        "train_tiles, train_perms = make_tile_group(out_tiles_shape, TRAIN_TILE_COUNT)\n",
        "\n",
        "valid_tiles, valid_perms = make_tile_group(out_tiles_shape, VALID_TILE_COUNT, is_validation = True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyZCoWYcx4E6"
      },
      "source": [
        "[Back to summary.](#topo)\n",
        "\n",
        "\n",
        "### Part 5: Implementing the Jigsaw model <a class=\"anchor\" id=\"part_05\"></a>\n",
        "\n",
        "\n",
        "<img src=\"http://www.ic.unicamp.br/~msreis/MC959/Jigsaw-03.png\">\n",
        "\n",
        "We'll implement an architecture very similar to the one in the paper. First, we'll implement an encoder for the patches. This encoder makes use of batch normalization, convolutional layers, max pooling layers, and LeakyReLU activation:\n",
        "\n",
        "<img src=\"https://pytorch.org/docs/stable/_images/LeakyReLU.png\">\n",
        "                                                               \n",
        "In the sequence, we'll implement a \"big model\", which concatenates all the encoded patches and, after some hidden layers, ends in a softmax with gives the probability that a given scrambled set of patches belong to a given permutation.\n",
        "                                              "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMdczq2wx4E7"
      },
      "outputs": [],
      "source": [
        "LATENT_SIZE = 8\n",
        "\n",
        "# Keras is for Tensorflow as PyTorch Lightining is for PyTorch.\n",
        "#\n",
        "from keras import models, layers\n",
        "\n",
        "# Instantiating a Keras model.\n",
        "#\n",
        "tile_encoder = models.Sequential(name = 'TileEncoder')\n",
        "\n",
        "# We use None to make the model more usuable later\n",
        "# (i.e., to have input size flexibility).\n",
        "#\n",
        "# Exercise: Put input_shape=(100,100) and analyze\n",
        "# the impact on the summary of the model.\n",
        "#\n",
        "tile_encoder.add(layers.BatchNormalization(input_shape=(None, None) + (train_tiles.shape[-1],)))\n",
        "\n",
        "tile_encoder.add(layers.Conv2D(8, (3,3), padding='same', activation='linear'))\n",
        "\n",
        "tile_encoder.add(layers.BatchNormalization())\n",
        "\n",
        "tile_encoder.add(layers.MaxPool2D(2,2))\n",
        "\n",
        "tile_encoder.add(layers.LeakyReLU(0.1))\n",
        "\n",
        "tile_encoder.add(layers.Conv2D(16, (3,3), padding='same', activation='linear'))\n",
        "\n",
        "tile_encoder.add(layers.BatchNormalization())\n",
        "\n",
        "tile_encoder.add(layers.MaxPool2D(2,2))\n",
        "\n",
        "tile_encoder.add(layers.LeakyReLU(0.1))\n",
        "\n",
        "tile_encoder.add(layers.Conv2D(32, (2,2), padding='valid', activation='linear'))\n",
        "\n",
        "tile_encoder.add(layers.BatchNormalization())\n",
        "\n",
        "tile_encoder.add(layers.LeakyReLU(0.1))\n",
        "\n",
        "tile_encoder.add(layers.Conv2D(LATENT_SIZE, (1,1), activation='linear'))\n",
        "\n",
        "tile_encoder.add(layers.BatchNormalization())\n",
        "\n",
        "tile_encoder.add(layers.LeakyReLU(0.1))\n",
        "\n",
        "clear_output() # some annoying loading/warnings come up\n",
        "\n",
        "tile_encoder.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFr95IQVx4E7"
      },
      "outputs": [],
      "source": [
        "# Model for a single tile.\n",
        "#\n",
        "print('Model Input Shape:', train_tiles.shape[2:],\n",
        "      '-> Model Output Shape:', tile_encoder.predict(np.zeros((1,) + train_tiles.shape[2:])).shape[1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB1ypShix4E7"
      },
      "source": [
        "\n",
        "Now, let's implement the model for concat of encoded patches and the assignment of the probability of their scramble being being each of the considered permutations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JT2vQ_-Nx4E7"
      },
      "outputs": [],
      "source": [
        "!pip install pyparsing==3.0.9\n",
        "!pip install pydot\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from IPython.display import Image\n",
        "\n",
        "BIG_LATENT_SIZE  = 64\n",
        "\n",
        "# Input here includes all tiles.\n",
        "#\n",
        "big_in = layers.Input(train_tiles.shape[1:], name='All_Tile_Input')\n",
        "\n",
        "feat_vec = []\n",
        "\n",
        "for k in range(train_tiles.shape[1]):  # shape[1] is the number of tiles.\n",
        "\n",
        "    lay_x     = layers.Lambda(lambda x: x[:, k], name = 'Select_{}_Tile'.format(k))(big_in)\n",
        "    feat_x    = tile_encoder(lay_x)\n",
        "    feat_vec += [layers.GlobalAvgPool2D()(feat_x)]\n",
        "\n",
        "feat_cat = layers.concatenate(feat_vec)\n",
        "\n",
        "feat_dr = layers.Dropout(0.5)(feat_cat)\n",
        "\n",
        "feat_latent = layers.Dense(BIG_LATENT_SIZE)(feat_dr)\n",
        "\n",
        "feat_latent_dr = layers.Dropout(0.5)(feat_latent)\n",
        "\n",
        "out_pred = layers.Dense(KEEP_RANDOM_PERM,\n",
        "                        activation = 'softmax')(feat_latent_dr)\n",
        "\n",
        "big_model = models.Model(inputs = [big_in], outputs = [out_pred])\n",
        "\n",
        "big_model.compile(optimizer = 'adam',\n",
        "                  loss = 'sparse_categorical_crossentropy',\n",
        "                  metrics = ['sparse_categorical_accuracy', 'sparse_top_k_categorical_accuracy'])\n",
        "\n",
        "# Plotting and saving image of the model's architecture.\n",
        "#\n",
        "dot_model = model_to_dot(big_model, show_shapes=True)\n",
        "dot_model.set_rankdir('LR')\n",
        "dot_model.write_png('Jigsaw_model.png')\n",
        "Image(dot_model.create_png())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOJHcHm0x4E8"
      },
      "source": [
        "Here, we compute a reverse mapping for the permutations (an \"unscramble function\"), so we can show the prediction results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TE8N3fHMx4E8"
      },
      "outputs": [],
      "source": [
        "reversed_keep_perm = []\n",
        "\n",
        "for c_dict in [{j: i for i, j in enumerate(c_perm)} for c_perm in keep_perm]:\n",
        "    print(c_dict)\n",
        "    reversed_keep_perm.append([c_dict[j] for j in range(out_tiles.shape[0])])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRFSaVKIx4E8"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "    print('forward', keep_perm[i], 'reversed', reversed_keep_perm[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vINYlV4ix4E8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def show_model_output(image_count=4, perm_count=3):\n",
        "\n",
        "    fig, m_axs = plt.subplots(image_count, perm_count + 1,\n",
        "                              figsize = (3 * (perm_count + 1), 3 * image_count))\n",
        "    [c_ax.axis('off') for c_ax in m_axs.flatten()]\n",
        "\n",
        "    for img_idx, c_axs in enumerate(m_axs):\n",
        "\n",
        "        # Sample an image for input.\n",
        "        #\n",
        "        img_idx = np.random.choice(range(X_train.shape[0]))\n",
        "\n",
        "        # Sample a permutation.\n",
        "        #\n",
        "        perm_idx = np.random.choice(range(len(keep_perm)))\n",
        "\n",
        "        # Plot sampled image.\n",
        "        #\n",
        "        c_axs[0].imshow(X_train[img_idx, :, :, 0])\n",
        "        c_axs[0].set_title('Input #{}'.format(perm_idx))\n",
        "\n",
        "        # Generate tiles with sampled image.\n",
        "        #\n",
        "        out_tiles = cut_jigsaw(X_train[img_idx, :, :], TILE_X, TILE_Y, gap = True,\n",
        "                               jitter = JITTER_SIZE>0, jitter_dim = JITTER_SIZE)\n",
        "\n",
        "        # Scramble tiles with sampled permutation.\n",
        "        #\n",
        "        c_perm = keep_perm[perm_idx]\n",
        "        scr_tiles = out_tiles[c_perm]\n",
        "\n",
        "        # Get model prediction for the given scrambled tiles.\n",
        "        #\n",
        "        out_pred = big_model.predict(np.expand_dims(scr_tiles, 0))[0]\n",
        "\n",
        "        # Plot the scramble image and the most probable permutation index\n",
        "        # according to our model (softmax over KEEP_RANDOM_PERM; e.g., 200).\n",
        "        #\n",
        "        for c_ax, k_idx in zip(c_axs[1:], np.argsort(-1 * out_pred)):\n",
        "            pred_rev_perm = reversed_keep_perm[k_idx]\n",
        "            recon_img = jigsaw_to_image(scr_tiles[pred_rev_perm], X_train.shape[1], X_train.shape[2])\n",
        "            c_ax.imshow(recon_img[:, :, 0])\n",
        "            c_ax.set_title('Pred: #{} ({:2.2%})'.format(k_idx, out_pred[k_idx]))\n",
        "\n",
        "show_model_output()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wX84fd7x4E9"
      },
      "source": [
        "[Back to summary.](#topo)\n",
        "\n",
        "### Part 6: Self-supervised pretraining with the Jigasaw and MNIST <a class=\"anchor\" id=\"part_06\"></a>\n",
        "\n",
        "Finally, we can proceed and carry out the pretext task!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQpGursXx4E9"
      },
      "outputs": [],
      "source": [
        "# Training the model.\n",
        "#\n",
        "fit_results = big_model.fit(train_tiles,\n",
        "                            train_perms,\n",
        "                            validation_data = (valid_tiles, valid_perms),\n",
        "                            batch_size = 512,\n",
        "                            epochs = 40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoBtc_j2x4E9"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\n",
        "ax1.semilogy(fit_results.history['loss'], label='Training')\n",
        "ax1.semilogy(fit_results.history['val_loss'], label='Validation')\n",
        "ax1.legend()\n",
        "ax1.set_title('Loss')\n",
        "ax2.plot(fit_results.history['sparse_categorical_accuracy'], label='Training')\n",
        "ax2.plot(fit_results.history['val_sparse_categorical_accuracy'], label='Validation')\n",
        "ax2.legend()\n",
        "ax2.set_title('Accuracy')\n",
        "ax2.set_ylim(0, 1)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTMhCnD9x4E-"
      },
      "outputs": [],
      "source": [
        "show_model_output(image_count=10, perm_count=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu5sh1qnx4E-"
      },
      "source": [
        "[Back to summary.](#topo)\n",
        "\n",
        "### Part 7: Exploration of the learned representations <a class=\"anchor\" id=\"part_07\"></a>\n",
        "\n",
        "Now, we can evaluate the representations we have just learned from unlabeled data. To this end, can look at the filters, the activations and also perform a non-linear dimensionaly reduction using t-SNE.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NzJOKccx4E-"
      },
      "source": [
        "#### Looking at the filters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kjWn_z_x4E-"
      },
      "outputs": [],
      "source": [
        "# Assemble a dictionary to store the weights having\n",
        "# the layer index and name as keys.\n",
        "#\n",
        "conv_weight_dict = {(idx, k.name): k.get_weights() for idx, k in enumerate(tile_encoder.layers)\n",
        "                    if isinstance(k, layers.Conv2D)}\n",
        "print(conv_weight_dict.keys())\n",
        "\n",
        "fig, m_axs = plt.subplots(2, 2, figsize=(20, 20))\n",
        "\n",
        "for c_ax, ((idx, lay_name), [W, b]) in zip(m_axs.flatten(), conv_weight_dict.items()):\n",
        "\n",
        "    c_ax.set_title('{} #{}\\n{}'.format(lay_name, idx, W.shape))\n",
        "\n",
        "    flat_W = W.reshape((W.shape[0], W.shape[1], -1)).swapaxes(0, 2).swapaxes(1,2)\n",
        "\n",
        "    if flat_W.shape[1] > 1 or flat_W.shape[2] > 1:\n",
        "        pad_W = np.pad(flat_W, [(0, 0), (1, 1), (1, 1)], mode = 'constant', constant_values = np.NAN)\n",
        "        pad_W = montage(pad_W, fill = np.NAN, grid_shape = (W.shape[2], W.shape[3]))\n",
        "\n",
        "    else:\n",
        "        pad_W = W[0, 0]\n",
        "\n",
        "    c_ax.imshow(pad_W, vmin = -1, vmax = 1, cmap = 'RdBu')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ml_L-9zx4E-"
      },
      "source": [
        "#### Looking at the most activating imaging channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRgR6v8Px4E_"
      },
      "outputs": [],
      "source": [
        "gp_outputs = []\n",
        "\n",
        "for k in tile_encoder.layers:\n",
        "\n",
        "    # Look for activation layers only (in this case, the leaky ReLU),\n",
        "    #\n",
        "    if isinstance(k, layers.LeakyReLU):\n",
        "\n",
        "        c_output = k.get_output_at(0)\n",
        "\n",
        "        c_smooth = layers.AvgPool2D((2, 2))(c_output)\n",
        "\n",
        "        c_gp = layers.GnvlobalMaxPool2D(name='GP_{}'.format(k.name))(c_smooth)\n",
        "\n",
        "        gp_outputs += [c_gp]\n",
        "\n",
        "activation_tile_encoder = models.Model(inputs = tile_encoder.inputs, outputs = gp_outputs)\n",
        "\n",
        "activation_tile_encoder.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bQdDaIqx4FF"
      },
      "outputs": [],
      "source": [
        "activation_maps = dict(zip(activation_tile_encoder.output_names,\n",
        "                           activation_tile_encoder.predict(X_train, batch_size = 128, verbose = True)))\n",
        "\n",
        "for k, v in activation_maps.items():\n",
        "    print(k, v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8wsOtafx4FF"
      },
      "source": [
        "Here we show each intermediate layer (panel) with each neuron/depth-channel (row) and the top-n images for activating that pattern (columns). Each row should more or less represent the kinds of images that particular neuron is sensitive too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwBfQRqtx4FG"
      },
      "outputs": [],
      "source": [
        "keep_top_n = 5\n",
        "\n",
        "fig, m_axs = plt.subplots(1, len(activation_maps), figsize=(20, 20))\n",
        "\n",
        "for c_ax, (k, v) in zip(m_axs.T, activation_maps.items()):\n",
        "\n",
        "    c_ax.set_title(k)\n",
        "    active_rows = []\n",
        "\n",
        "    for i in range(v.shape[1]):\n",
        "\n",
        "        # Gets the indices of the top n images and stores the respective images.\n",
        "        #\n",
        "        top_idx = np.argsort(-np.abs(v[:, i]))[:keep_top_n]\n",
        "        active_rows += [X_train[top_idx, :, :, 0]]\n",
        "\n",
        "    # Plot all rows for a given activation.\n",
        "    #\n",
        "    c_ax.imshow(montage(np.concatenate(active_rows, 0),\n",
        "                        grid_shape = (v.shape[1], keep_top_n),\n",
        "                        padding_width = 1))\n",
        "    c_ax.axis('off')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62rw4OTNx4FG"
      },
      "source": [
        "#### Calculating t-SNE\n",
        "\n",
        "We can see whether t-SNE can separate the digits in the projection space. To this end, we throw in global average pooling to turn the output of the `tile_encoder` into a single feature-vector. We can also use this feature vector later for image classification (downstream task)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pijbAZnAx4FG"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(X_train[0].shape, '->', tile_encoder.predict(X_train[0:1]).shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lc2IUR62x4FG"
      },
      "outputs": [],
      "source": [
        "\n",
        "# img_in is an input layer with shape (28, 28, 1).\n",
        "#\n",
        "img_in = layers.Input(X_train.shape[1:])\n",
        "\n",
        "# Generating a representation (with shape 6,6,8) using the (single) tile encoder.\n",
        "#\n",
        "full_feat_mat = tile_encoder(img_in)\n",
        "\n",
        "# Reducing the representation to a 8-dimensional vector.\n",
        "#\n",
        "gap_out = layers.GlobalAvgPool2D()(full_feat_mat)\n",
        "\n",
        "# Defining our final image encoder.\n",
        "#\n",
        "image_encoder = models.Model(inputs = [img_in], outputs = [gap_out], name = 'EncodeImage')\n",
        "image_encoder.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m96cb5dNx4FG"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_features = image_encoder.predict(X_train, batch_size = 128)\n",
        "\n",
        "X_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PJfzA0mx4FG"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(n_components = 2, perplexity = 40, verbose = 2, n_iter = 250, early_exaggeration = 1)\n",
        "\n",
        "X_tsne = tsne.fit_transform(X_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjISULXjx4FG"
      },
      "outputs": [],
      "source": [
        "fig, ax1 = plt.subplots(1, 1, figsize=(20, 20))\n",
        "for k in np.unique(y_train):\n",
        "    ax1.plot(X_tsne[y_train==k, 0], X_tsne[y_train==k, 1], '.', label='{}'.format(k))\n",
        "ax1.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTb83H5ex4FG"
      },
      "source": [
        "[Back to summary.](#topo)\n",
        "\n",
        "### Part 8: Evaluating the downstream task (digit classification) <a class=\"anchor\" id=\"part_08\"></a>\n",
        "\n",
        "We'll evaluate now how our learned representation perform on a downstream task. The baseline will be a classification based on pixels. We'll explore two different models: logistic regression and random forest.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MW7jqzAkx4FG"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "X_features = image_encoder.predict(X_down, batch_size = 128)\n",
        "\n",
        "train_imgs, test_imgs, train_feat, test_feat, train_y, test_y = train_test_split(\n",
        "        X_down,\n",
        "        X_features,\n",
        "        y_down,\n",
        "        random_state = MY_SEED,\n",
        "        test_size    = 0.1,\n",
        "        stratify     = y_down\n",
        ")\n",
        "\n",
        "print(train_imgs.shape, train_feat.shape, train_y.shape)\n",
        "print(test_imgs.shape, test_feat.shape, test_y.shape)\n",
        "\n",
        "\n",
        "def train_and_show_model(in_model, train_size, random_state, show_figure = True):\n",
        "\n",
        "    # Fit pixel model.\n",
        "    #\n",
        "    baseline_model = make_pipeline(Normalizer(), in_model)\n",
        "    baseline_model.fit(train_imgs[0:train_size].reshape((-1, 784)), train_y[0:train_size])\n",
        "    baseline_pred_y = baseline_model.predict(test_imgs.reshape((-1, 784)))\n",
        "\n",
        "    # Fit feature model.\n",
        "    #\n",
        "    feat_model = make_pipeline(Normalizer(), in_model)\n",
        "    feat_model.fit(train_feat[0:train_size], train_y[0:train_size])\n",
        "    pred_y = feat_model.predict(test_feat)\n",
        "\n",
        "    if show_figure:\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "        sns.heatmap(confusion_matrix(test_y, baseline_pred_y), annot=True, fmt='d', ax=ax1)\n",
        "        ax1.set_title('Pixel Accuracy: {:2.1%}'.format(accuracy_score(test_y, baseline_pred_y)))\n",
        "\n",
        "        sns.heatmap(confusion_matrix(test_y, pred_y), annot=True, fmt='d', ax=ax2)\n",
        "        ax2.set_title('Feature Accuracy: {:2.1%}'.format(accuracy_score(test_y, pred_y)))\n",
        "\n",
        "    return {\n",
        "        'train_size': train_size,\n",
        "        'random_state': random_state,\n",
        "        'pixel_accuracy': accuracy_score(test_y, baseline_pred_y),\n",
        "        'feature_accuracy': accuracy_score(test_y, pred_y)\n",
        "    }\n",
        "\n",
        "print('[Done]')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StVL_de1x4FH"
      },
      "source": [
        "#### Logistic regression (linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9j9SoK3x4FH"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_and_show_model(LogisticRegression(solver='lbfgs', multi_class='auto'), 9000, MY_SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK3mab5Gx4FH"
      },
      "source": [
        "#### Random forest (non-linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JSyQeA1x4FH"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_and_show_model(RandomForestClassifier(n_estimators=100), 9000, MY_SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmM6tR-px4FH"
      },
      "source": [
        "#### Evaluating the models with few data\n",
        "\n",
        "Here we evaluate the impact of the downstream training set size on the performance of the random forest, for both pixel and vector classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBBviaFzx4FH"
      },
      "outputs": [],
      "source": [
        "import dask\n",
        "from dask import bag\n",
        "import dask.diagnostics as diag\n",
        "from multiprocessing.pool import ThreadPool\n",
        "import gc; gc.enable(); gc.collect() # memory gets very tight\n",
        "\n",
        "model_fn = lambda : RandomForestClassifier(n_estimators = 25, random_state = MY_SEED, n_jobs = 1)\n",
        "\n",
        "# We vary the number of samples between 1 and 1000, and the seed value between 0 and 9\n",
        "# (for different initializations, in order to obtain a mean and a deviation).\n",
        "#\n",
        "parm_sweep = bag.\\\n",
        "    from_sequence(product(np.logspace(1, 3, 10).astype(int), # sample size\n",
        "                          np.arange(10))).\\\n",
        "    map(lambda args: train_and_show_model(model_fn(),\n",
        "                                     train_size=args[0],\n",
        "                                     random_state=args[1],\n",
        "                                    show_figure=False))\n",
        "parm_sweep\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4HouZsfx4FH"
      },
      "outputs": [],
      "source": [
        "with diag.ProgressBar(), dask.config.set(pool = ThreadPool(2)):\n",
        "    parm_df = pd.DataFrame(parm_sweep.compute())\n",
        "\n",
        "# clean-up the output\n",
        "#\n",
        "nice_parm_df = pd.melt(parm_df, id_vars = ['train_size', 'random_state']).\\\n",
        "    rename(columns={'value': 'Test Accuracy', 'variable': 'input'})\n",
        "\n",
        "nice_parm_df['Test Accuracy'] *= 100\n",
        "\n",
        "nice_parm_df['input'] = nice_parm_df['input'].map(lambda x: x.split('_accuracy')[0])\n",
        "\n",
        "nice_parm_df['Samples Per Class'] = nice_parm_df['train_size'] / np.unique(y_train).shape[0]\n",
        "\n",
        "nice_parm_df.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PotHHyyox4FI"
      },
      "outputs": [],
      "source": [
        "sns.catplot(x        = 'Samples Per Class',\n",
        "            y        = 'Test Accuracy',\n",
        "            hue      = 'input',\n",
        "            kind     = 'point',\n",
        "            errorbar = 'sd',\n",
        "            data     = nice_parm_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWF_o3F3x4FI"
      },
      "outputs": [],
      "source": [
        "sns.catplot(x='Samples Per Class',\n",
        "            y='Test Accuracy',\n",
        "            hue='input',\n",
        "            kind='swarm',\n",
        "            data=nice_parm_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZQq-E40x4FI"
      },
      "source": [
        "[Back to summary.](#topo)\n",
        "\n",
        "\n",
        "### Part 9: Sugestions of exercises on this pipeline <a class=\"anchor\" id=\"part_09\"></a>\n",
        "\n",
        "- Try to explore hyperparameters of the pretext task;\n",
        "\n",
        "- Run this pipeline on other dataset (e.g., STL-10);\n",
        "\n",
        "- Try to implement the permutation algorithm of the Jigsaw paper:\n",
        "\n",
        "<img src=\"http://www.ic.unicamp.br/~msreis/MC959/Jigsaw-04.png\">\n",
        "\n",
        "- Try to port this pipeline to PyTorch.\n",
        "\n",
        "\n",
        "[Back to summary.](#topo)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "databundleVersionId": 861823,
          "sourceId": 3004,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 29186,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python (env)",
      "language": "python",
      "name": "python-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
